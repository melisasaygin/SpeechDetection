{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "900c4a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the needed libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, recall_score, f1_score, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612cdf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccZ_Spectral_Variance</th>\n",
       "      <th>AccZ_RMS</th>\n",
       "      <th>AccZ_Spectral_Skewness</th>\n",
       "      <th>AccZ_Spectral_Mean</th>\n",
       "      <th>AccZ_Spectral_Kurtosis</th>\n",
       "      <th>AccZ_Spectral_Flatness</th>\n",
       "      <th>AccZ_Spectral_Entropy</th>\n",
       "      <th>AccZ_Spectral_Crest</th>\n",
       "      <th>AccZ_ZCR</th>\n",
       "      <th>AccZ_Spectral_Rolloff_50</th>\n",
       "      <th>Task_Label</th>\n",
       "      <th>Participant</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009647</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.638555</td>\n",
       "      <td>0.160822</td>\n",
       "      <td>0.348008</td>\n",
       "      <td>0.344127</td>\n",
       "      <td>13.584974</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.527733</td>\n",
       "      <td>272.957495</td>\n",
       "      <td>12.0a</td>\n",
       "      <td>10785</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009625</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.568576</td>\n",
       "      <td>0.162448</td>\n",
       "      <td>0.094564</td>\n",
       "      <td>0.345113</td>\n",
       "      <td>13.588027</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.525300</td>\n",
       "      <td>271.757481</td>\n",
       "      <td>12.0b</td>\n",
       "      <td>10785</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009874</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.607381</td>\n",
       "      <td>0.163459</td>\n",
       "      <td>0.144821</td>\n",
       "      <td>0.345735</td>\n",
       "      <td>13.588096</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.524629</td>\n",
       "      <td>271.898173</td>\n",
       "      <td>12.0c</td>\n",
       "      <td>10785</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010136</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.649654</td>\n",
       "      <td>0.164157</td>\n",
       "      <td>0.329661</td>\n",
       "      <td>0.340929</td>\n",
       "      <td>13.582662</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.528676</td>\n",
       "      <td>272.618181</td>\n",
       "      <td>12.0d</td>\n",
       "      <td>10785</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009663</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.652239</td>\n",
       "      <td>0.161347</td>\n",
       "      <td>0.356853</td>\n",
       "      <td>0.346463</td>\n",
       "      <td>13.588352</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.522279</td>\n",
       "      <td>270.532640</td>\n",
       "      <td>12.0e</td>\n",
       "      <td>10785</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <td>0.012422</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.744622</td>\n",
       "      <td>0.178976</td>\n",
       "      <td>0.806176</td>\n",
       "      <td>0.328023</td>\n",
       "      <td>13.575815</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.516833</td>\n",
       "      <td>265.476033</td>\n",
       "      <td>85</td>\n",
       "      <td>94527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>0.113360</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>3.133800</td>\n",
       "      <td>0.329863</td>\n",
       "      <td>13.855738</td>\n",
       "      <td>0.196810</td>\n",
       "      <td>13.331840</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.429472</td>\n",
       "      <td>214.338917</td>\n",
       "      <td>87</td>\n",
       "      <td>94527</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>0.027212</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>2.498372</td>\n",
       "      <td>0.202672</td>\n",
       "      <td>11.005391</td>\n",
       "      <td>0.312232</td>\n",
       "      <td>13.469382</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.506670</td>\n",
       "      <td>263.117386</td>\n",
       "      <td>89</td>\n",
       "      <td>94527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>0.060207</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>2.645358</td>\n",
       "      <td>0.300402</td>\n",
       "      <td>14.623167</td>\n",
       "      <td>0.243584</td>\n",
       "      <td>13.466717</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.453911</td>\n",
       "      <td>238.066075</td>\n",
       "      <td>91</td>\n",
       "      <td>94527</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>0.014346</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>1.182004</td>\n",
       "      <td>0.180541</td>\n",
       "      <td>3.022856</td>\n",
       "      <td>0.319676</td>\n",
       "      <td>13.553092</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.517239</td>\n",
       "      <td>265.831899</td>\n",
       "      <td>93</td>\n",
       "      <td>94527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2706 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AccZ_Spectral_Variance  AccZ_RMS  AccZ_Spectral_Skewness  \\\n",
       "0                   0.009647  0.001076                0.638555   \n",
       "1                   0.009625  0.001083                0.568576   \n",
       "2                   0.009874  0.001092                0.607381   \n",
       "3                   0.010136  0.001098                0.649654   \n",
       "4                   0.009663  0.001078                0.652239   \n",
       "...                      ...       ...                     ...   \n",
       "2701                0.012422  0.001175                0.744622   \n",
       "2702                0.113360  0.002589                3.133800   \n",
       "2703                0.027212  0.001323                2.498372   \n",
       "2704                0.060207  0.002128                2.645358   \n",
       "2705                0.014346  0.001193                1.182004   \n",
       "\n",
       "      AccZ_Spectral_Mean  AccZ_Spectral_Kurtosis  AccZ_Spectral_Flatness  \\\n",
       "0               0.160822                0.348008                0.344127   \n",
       "1               0.162448                0.094564                0.345113   \n",
       "2               0.163459                0.144821                0.345735   \n",
       "3               0.164157                0.329661                0.340929   \n",
       "4               0.161347                0.356853                0.346463   \n",
       "...                  ...                     ...                     ...   \n",
       "2701            0.178976                0.806176                0.328023   \n",
       "2702            0.329863               13.855738                0.196810   \n",
       "2703            0.202672               11.005391                0.312232   \n",
       "2704            0.300402               14.623167                0.243584   \n",
       "2705            0.180541                3.022856                0.319676   \n",
       "\n",
       "      AccZ_Spectral_Entropy  AccZ_Spectral_Crest  AccZ_ZCR  \\\n",
       "0                 13.584974             0.000274  0.527733   \n",
       "1                 13.588027             0.000262  0.525300   \n",
       "2                 13.588096             0.000239  0.524629   \n",
       "3                 13.582662             0.000272  0.528676   \n",
       "4                 13.588352             0.000289  0.522279   \n",
       "...                     ...                  ...       ...   \n",
       "2701              13.575815             0.000298  0.516833   \n",
       "2702              13.331840             0.000672  0.429472   \n",
       "2703              13.469382             0.000559  0.506670   \n",
       "2704              13.466717             0.000794  0.453911   \n",
       "2705              13.553092             0.000393  0.517239   \n",
       "\n",
       "      AccZ_Spectral_Rolloff_50 Task_Label  Participant  Classification  \n",
       "0                   272.957495      12.0a        10785               0  \n",
       "1                   271.757481      12.0b        10785               0  \n",
       "2                   271.898173      12.0c        10785               0  \n",
       "3                   272.618181      12.0d        10785               0  \n",
       "4                   270.532640      12.0e        10785               0  \n",
       "...                        ...        ...          ...             ...  \n",
       "2701                265.476033         85        94527               0  \n",
       "2702                214.338917         87        94527               1  \n",
       "2703                263.117386         89        94527               0  \n",
       "2704                238.066075         91        94527               1  \n",
       "2705                265.831899         93        94527               0  \n",
       "\n",
       "[2706 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading in the data we will use in the machine learning pipeline, with the top ten features of the method\n",
    "df = pd.read_excel('pro_accgyro_training_set_10_final.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "897f50b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[9.64670000e-03, 1.07550000e-03, 6.38554700e-01, ...,\n",
       "         2.74400000e-04, 5.27732700e-01, 2.72957495e+02],\n",
       "        [9.62510000e-03, 1.08310000e-03, 5.68575600e-01, ...,\n",
       "         2.62400000e-04, 5.25299600e-01, 2.71757481e+02],\n",
       "        [9.87380000e-03, 1.09150000e-03, 6.07381000e-01, ...,\n",
       "         2.39100000e-04, 5.24629200e-01, 2.71898173e+02],\n",
       "        ...,\n",
       "        [2.72120000e-02, 1.32320000e-03, 2.49837230e+00, ...,\n",
       "         5.58800000e-04, 5.06670400e-01, 2.63117386e+02],\n",
       "        [6.02072000e-02, 2.12800000e-03, 2.64535850e+00, ...,\n",
       "         7.94300000e-04, 4.53911200e-01, 2.38066075e+02],\n",
       "        [1.43461000e-02, 1.19270000e-03, 1.18200380e+00, ...,\n",
       "         3.93000000e-04, 5.17238800e-01, 2.65831899e+02]]),\n",
       " 'target': array([0, 0, 0, ..., 0, 1, 0]),\n",
       " 'feature_names': ['AccZ_Spectral_Variance',\n",
       "  'AccZ_RMS',\n",
       "  'AccZ_Spectral_Skewness',\n",
       "  'AccZ_Spectral_Mean',\n",
       "  'AccZ_Spectral_Kurtosis',\n",
       "  'AccZ_Spectral_Flatness',\n",
       "  'AccZ_Spectral_Entropy',\n",
       "  'AccZ_Spectral_Crest',\n",
       "  'AccZ_ZCR',\n",
       "  'AccZ_Spectral_Rolloff_50'],\n",
       " 'participants': array([10785, 10785, 10785, ..., 94527, 94527, 94527]),\n",
       " 'task': array(['12.0a', '12.0b', '12.0c', ..., 89, 91, 93], dtype=object)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting the features from the DataFrame\n",
    "features = df.drop(columns=['Classification', 'Participant', 'Task_Label'])\n",
    "\n",
    "# extracting the Classification (0/1) and participants from the dataframe\n",
    "target = df['Classification'].values\n",
    "participants = df['Participant'].values\n",
    "task = df['Task_Label'].values\n",
    "\n",
    "# extracting the feature names\n",
    "feature_names = features.columns.tolist()\n",
    "\n",
    "# converting the features, target, participants to numpy arrays\n",
    "features_array = features.values\n",
    "target_array = target\n",
    "\n",
    "# creating the dictionary with the needed format to enter into the nested cross-validation\n",
    "data_dict = {\n",
    "    'data': features_array,\n",
    "    'target': target_array,\n",
    "    'feature_names': feature_names,\n",
    "    'participants': participants,\n",
    "    'task': task\n",
    "}\n",
    "\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebd8e90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nested_accuracy_scores:    [0.92424242 0.98484848 0.98484848 1.         1.         1.\n",
      " 0.87878788 1.         0.96969697 1.         1.         1.\n",
      " 0.96969697 0.98484848 1.         1.         0.98484848 0.96969697\n",
      " 1.         1.         1.         0.87878788 1.         1.\n",
      " 1.         0.98484848 1.         1.         0.83333333 1.\n",
      " 1.         1.         1.         0.98484848 1.         1.\n",
      " 1.         0.90909091 0.78787879 1.         1.        ]\n",
      "mean score:            0.9763\n",
      "nested_precision_scores:    [0.93230174 0.98520085 0.98520085 1.         1.         1.\n",
      " 0.89747475 1.         0.97107438 1.         1.         1.\n",
      " 0.96969697 0.98545455 1.         1.         0.98545455 0.97107438\n",
      " 1.         1.         1.         0.89818182 1.         1.\n",
      " 1.         0.98545455 1.         1.         0.88571429 1.\n",
      " 1.         1.         1.         0.98545455 1.         1.\n",
      " 1.         0.92045455 0.78787879 1.         1.        ]\n",
      "mean score:            0.9792\n",
      "nested_recall_scores:    [0.92424242 0.98484848 0.98484848 1.         1.         1.\n",
      " 0.87878788 1.         0.96969697 1.         1.         1.\n",
      " 0.96969697 0.98484848 1.         1.         0.98484848 0.96969697\n",
      " 1.         1.         1.         0.87878788 1.         1.\n",
      " 1.         0.98484848 1.         1.         0.83333333 1.\n",
      " 1.         1.         1.         0.98484848 1.         1.\n",
      " 1.         0.90909091 0.78787879 1.         1.        ]\n",
      "mean score:            0.9763\n",
      "nested_f1_scores:    [0.91377058 0.98347935 0.98347935 1.         1.         1.\n",
      " 0.87464387 1.         0.96663296 1.         1.         1.\n",
      " 0.9672619  0.98377182 1.         1.         0.98377182 0.96663296\n",
      " 1.         1.         1.         0.85652174 1.         1.\n",
      " 1.         0.98377182 1.         1.         0.8314372  1.\n",
      " 1.         1.         1.         0.98377182 1.         1.\n",
      " 1.         0.8952381  0.77083333 1.         1.        ]\n",
      "mean score:            0.9743\n",
      "nested_roc_auc_scores:    [0.87797619 1.         1.         1.         1.         1.\n",
      " 0.96130952 1.         1.         1.         1.         1.\n",
      " 0.99900794 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.99801587 1.         1.\n",
      " 1.         1.         1.         1.         0.96924603 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.82787698 1.         1.        ]\n",
      "mean score:            0.9911\n"
     ]
    }
   ],
   "source": [
    "#running the nested cross-validation for the Accelerometer method using XGBoost\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# random seed for reproducibility\n",
    "random_seed = 1\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "X = data_dict['data']\n",
    "Y = data_dict['target']\n",
    "groups = data_dict['participants']\n",
    "\n",
    "# setting up the parameter grid: the combinations of these will be tried out for ever outer loop's inner loop\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'subsample': [0.9, 1.0],\n",
    "    'max_depth': [3, 4],\n",
    "    'min_child_weight': [1, 2],\n",
    "    'gamma': [0, 1],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "\n",
    "xgbc = xgb.XGBClassifier(random_state=random_seed)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "\n",
    "# defining the techniques to use for the inner and outer loops\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "outer_cv = logo.split(X, Y, groups)\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1_score': make_scorer(f1_score, average='macro'),\n",
    "    'roc_auc': make_scorer(roc_auc_score, needs_proba=True, average='weighted')\n",
    "}\n",
    "\n",
    "# performing nested cross-validation\n",
    "clf = GridSearchCV(estimator=xgbc, param_grid=param_grid, cv=inner_cv)\n",
    "cv_dic = cross_validate(clf, X, Y, cv=outer_cv, scoring=scoring, return_estimator=True, return_train_score=False)\n",
    "mean_acc_score = cv_dic['test_accuracy'].mean()\n",
    "mean_prec_score = cv_dic['test_precision'].mean()\n",
    "mean_rec_score = cv_dic['test_recall'].mean()\n",
    "mean_f1_score = cv_dic['test_f1_score'].mean()\n",
    "mean_roc_auc = cv_dic['test_roc_auc'].mean()  # Mean ROC AUC score\n",
    "\n",
    "\n",
    "print('nested_accuracy_scores:   ', cv_dic['test_accuracy'])\n",
    "print('mean score:            {0:.4f}'.format(mean_acc_score))\n",
    "\n",
    "print('nested_precision_scores:   ', cv_dic['test_precision'])\n",
    "print('mean score:            {0:.4f}'.format(mean_prec_score))\n",
    "\n",
    "print('nested_recall_scores:   ', cv_dic['test_recall'])\n",
    "print('mean score:            {0:.4f}'.format(mean_rec_score))\n",
    "\n",
    "print('nested_f1_scores:   ', cv_dic['test_f1_score'])\n",
    "print('mean score:            {0:.4f}'.format(mean_f1_score))\n",
    "\n",
    "print('nested_roc_auc_scores:   ', cv_dic['test_roc_auc'])\n",
    "print('mean score:            {0:.4f}'.format(mean_roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998c50d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Sensitivity: 0.7917\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 2:\n",
      "Sensitivity: 0.9583\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 3:\n",
      "Sensitivity: 0.9583\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 4:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 5:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 6:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 7:\n",
      "Sensitivity: 0.9583\n",
      "Specificity: 0.8333\n",
      "==============================\n",
      "Fold 8:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 9:\n",
      "Sensitivity: 0.9167\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 10:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 11:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 12:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 13:\n",
      "Sensitivity: 0.9583\n",
      "Specificity: 0.9762\n",
      "==============================\n",
      "Fold 14:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 0.9762\n",
      "==============================\n",
      "Fold 15:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 16:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 17:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 0.9762\n",
      "==============================\n",
      "Fold 18:\n",
      "Sensitivity: 0.9167\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 19:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 20:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 21:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 22:\n",
      "Sensitivity: 0.6667\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 23:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 24:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 25:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 26:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 0.9762\n",
      "==============================\n",
      "Fold 27:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 28:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 29:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 0.7381\n",
      "==============================\n",
      "Fold 30:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 31:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 32:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 33:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 34:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 0.9762\n",
      "==============================\n",
      "Fold 35:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 36:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 37:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 38:\n",
      "Sensitivity: 0.7500\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 39:\n",
      "Sensitivity: 0.7083\n",
      "Specificity: 0.8333\n",
      "==============================\n",
      "Fold 40:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Fold 41:\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 1.0000\n",
      "==============================\n",
      "Average Sensitivity: 0.9654471544715446\n",
      "Average Specificity: 0.9825783972125437\n"
     ]
    }
   ],
   "source": [
    "sensitivity_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "# iteration over the outer folds to calculate average sensitivity and specificity\n",
    "for i, (train_index, test_index) in enumerate(logo.split(X, Y, groups)):\n",
    "    print(f\"Fold {i + 1}:\")\n",
    "\n",
    "    # getting the trained estimator for the present fold\n",
    "    estimator = cv_dic['estimator'][i]\n",
    "    \n",
    "    # predicting on the test set\n",
    "    y_pred = estimator.predict(X[test_index])\n",
    "    \n",
    "    # creating confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(Y[test_index], y_pred).ravel()\n",
    "    \n",
    "    # calculating the sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # store the sensitivity and specificity scores\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    specificity_scores.append(specificity)\n",
    "\n",
    "    \n",
    "    # calculate and print the averages of the sensitivity and specificity scores previously calculated\n",
    "average_sensitivity = np.mean(sensitivity_scores)\n",
    "average_specificity = np.mean(specificity_scores)\n",
    "\n",
    "print(\"Average Sensitivity:\", average_sensitivity)\n",
    "print(\"Average Specificity:\", average_specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b66bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracy: Mean=0.9763488543976349, SD=0.04933543436164812, 95% CI=(0.9612475271509785, 0.9914501816442912), Min=0.7878787878787878, Max=1.0\n",
      "test_precision: Mean=0.979172476309615, SD=0.04357071504648762, 95% CI=(0.9658357005256095, 0.9925092520936205), Min=0.7878787878787878, Max=1.0\n",
      "test_recall: Mean=0.9763488543976349, SD=0.04933543436164812, 95% CI=(0.9612475271509785, 0.9914501816442912), Min=0.7878787878787878, Max=1.0\n",
      "test_f1_score: Mean=0.9742687471054666, SD=0.05324314685121377, 95% CI=(0.9579712888050489, 0.9905662054058844), Min=0.7708333333333334, Max=1.0\n",
      "test_roc_auc: Mean=0.9910593302361594, SD=0.03304786851528045, 95% CI=(0.9809435446205841, 1.0011751158517348), Min=0.8278769841269842, Max=1.0\n",
      "sensitivity_scores: Mean=0.9654471544715446, SD=0.08275971274841729, 95% CI=(0.9401148244490352, 0.9907794844940541), Min=0.6666666666666666, Max=1.0\n",
      "specificity_scores: Mean=0.9825783972125437, SD=0.053511710165413, 95% CI=(0.9661987330382618, 0.9989580613868256), Min=0.7380952380952381, Max=1.0\n"
     ]
    }
   ],
   "source": [
    "#print all performance scores that might be of interest\n",
    "import scipy.stats\n",
    "\n",
    "def calculate_stats(data):\n",
    "    mean = np.mean(data)\n",
    "    sd = np.std(data, ddof=1)  # using sample standard deviation (N-1)\n",
    "    ci = scipy.stats.norm.interval(0.95, loc=mean, scale=sd/np.sqrt(len(data)))\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    return mean, sd, ci, min_val, max_val\n",
    "\n",
    "keys = ['test_accuracy', 'test_precision', 'test_recall', 'test_f1_score', 'test_roc_auc', 'sensitivity_scores', 'specificity_scores']\n",
    "\n",
    "for key in keys:\n",
    "    if key in cv_dic:\n",
    "        data = cv_dic[key]\n",
    "    elif key == 'sensitivity_scores':\n",
    "        data = sensitivity_scores\n",
    "    elif key == 'specificity_scores':\n",
    "        data = specificity_scores\n",
    "\n",
    "    stats = calculate_stats(data)\n",
    "    print(f\"{key}: Mean={stats[0]}, SD={stats[1]}, 95% CI={stats[2]}, Min={stats[3]}, Max={stats[4]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "058bafd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Fold 2 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Fold 3 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Fold 4 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Fold 5 - Best hyperparameters: {'colsample_bytree': 1.0, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 6 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 7 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Fold 8 - Best hyperparameters: {'colsample_bytree': 1.0, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 9 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 10 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 11 - Best hyperparameters: {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 12 - Best hyperparameters: {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 13 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Fold 14 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Fold 15 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 16 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 17 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 18 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 19 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Fold 20 - Best hyperparameters: {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 21 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 22 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 2, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Fold 23 - Best hyperparameters: {'colsample_bytree': 1.0, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 24 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Fold 25 - Best hyperparameters: {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Fold 26 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 27 - Best hyperparameters: {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Fold 28 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 29 - Best hyperparameters: {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 30 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 0.9}\n",
      "Fold 31 - Best hyperparameters: {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Fold 32 - Best hyperparameters: {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Fold 33 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Fold 34 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Fold 35 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 36 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Fold 37 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 38 - Best hyperparameters: {'colsample_bytree': 1.0, 'gamma': 1, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 39 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 40 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n",
      "Fold 41 - Best hyperparameters: {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'subsample': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# IF this is the best-performing nested CV for the method: print best hyperparameters for each outer loop\n",
    "for fold_idx, estimator in enumerate(cv_dic['estimator']):\n",
    "    print(f'Fold {fold_idx + 1} - Best hyperparameters: {estimator.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "180394e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter: colsample_bytree - Most frequent value: 0.8 (Count: 29)\n",
      "Hyperparameter: gamma - Most frequent value: 0 (Count: 34)\n",
      "Hyperparameter: learning_rate - Most frequent value: 0.1 (Count: 41)\n",
      "Hyperparameter: max_depth - Most frequent value: 4 (Count: 28)\n",
      "Hyperparameter: min_child_weight - Most frequent value: 1 (Count: 37)\n",
      "Hyperparameter: n_estimators - Most frequent value: 200 (Count: 36)\n",
      "Hyperparameter: subsample - Most frequent value: 0.9 (Count: 26)\n"
     ]
    }
   ],
   "source": [
    "# IF this is the best-performing nested CV for the method: print mode of hyperparameters\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# initialize a dictionary to contain all hyperparameters and their values across loops\n",
    "hyperparameters = defaultdict(list)\n",
    "\n",
    "# extracting hyperparameters of each outer loop\n",
    "for estimator in cv_dic['estimator']:\n",
    "    for param, value in estimator.best_params_.items():\n",
    "        hyperparameters[param].append(value)\n",
    "\n",
    "# count the most frequent value for each hyperparameter\n",
    "for param, values in hyperparameters.items():\n",
    "    most_common_value, count = Counter(values).most_common(1)[0]  # Get the most frequent value\n",
    "    print(f'Hyperparameter: {param} - Most frequent value: {most_common_value} (Count: {count})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
